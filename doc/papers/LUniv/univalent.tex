\documentclass{llncs}

\usepackage{xcolor}
\usepackage{enumitem,amsmath,amssymb}
\usepackage{stmaryrd}    % needed for \mapsfrom
\usepackage{algorithm2e}
\usepackage{bussproofs}
\def\defaultHypSeparation{\hskip.1in}

\usepackage{tikz}
\usepackage{subfig}
\usepackage{array,booktabs,multirow}

\usepackage{logictools}
\usepackage{prooftheory}
\usepackage{comment}
\usepackage{mathenvironments}


\title{Propositional Resolution Proof Compression by Lowering Nodes}

\author{
  Joseph Boudou\inst{1}
  \thanks{This work was supported by the Google Summer of Code program.}
  \and 
  Bruno Woltzenlogel Paleo\inst{2}
}

\authorrunning{J.\~Boudou \and B.\~Woltzenlogel Paleo}

\institute{
  Universit\'e Paul Sabatier, Toulouse \\
  \email{joseph.boudou@matabio.net}
  \and 
  Vienna University of Technology \\
  \email{bruno@logic.at}
}




%\includeonly{LU_charts}
\begin{document}

\maketitle


\begin{abstract}
This paper describes a generalization of the {\LowerUnits} algorithm \cite{LURPI}
for the compression of propositional resolution proofs. 
The generalized algorithm, here called {\LowerUnivalents}, is
able to lower not only units but also proof nodes containing non-unit clauses, 
provided that their literals satisfy some additional conditions. 
A formal proof that {\LowerUnivalents} always compresses more than {\LowerUnits} is shown, 
and both algorithms are empirically compared on
thousands of proofs produced by the SMT-Solver \veriT.
\end{abstract}



\section{Introduction}

Sat-solvers are among the most successful automated deduction tools available today. As standalone
tools, they can already be applied to a wide range of problems, especially considering that, due to
the NP-completeness of propositional satisfiability \cite{cook}, any NP problem can be encoded as a
propositional formula. And, nevertheless, despite the theoretical difficulty associated with NP
problems, state-of-the-art sat-solving techniques perform surprisingly well in practice
\cite{sat-competition}. With the aim of leveraging this efficiency, sat-solvers have been embedded
into various other automated deduction tools that target problems described in more expressive
logics. The most prominent examples are SMT-solvers \cite{veriT}, for checking satisfiability modulo
theories for equality, linear arithmetic, bit-vectors, arrays and others. But more recently,
interactive proof assistants \cite{isabelle-blanchette-boehme} and automated first-order
\cite{spassT?MELIA?iProver?Vampire?} and even higher-order \cite{satallax} theorem provers have
taken advantage of embedding sat-solvers too.

In such a scenario, it is essential that sat-solvers output not only a \emph{yes} or \emph{no}
answer, but also a model (in case of satisfiability) or a refutation (in case of unsatisfiability).
For DPLL- and CDCL-based sat-solvers, propositional resolution is an excellent proof system, since
refutations in this system can be generated with an acceptable efficiency overhead and they are
detailed enough to allow easy implementation of efficient proof checkers.

ToDo: unsat core, interpolants...

With the increase in the demand for proofs from sat-solvers, there has been a surge of techniques to
compress and improve such proofs in a post-processing stage. ToDo: briefly describe and list related
work here.

In this paper...  ToDo: modify and expand the abstract here\\
 describes a generalization of the {\LowerUnits} algorithm \cite{LURPI} for the compression
of propositional resolution proofs. The generalized algorithm, here called {\LowerUnivalents}, is
able to lower not only units but also proof nodes containing non-unit clauses, provided that their
literals satisfy some additional conditions. A formal proof that {\LowerUnivalents} always
compresses more than {\LowerUnits} is shown, and both algorithms are empirically compared on
thousands of proofs produced by the SMT-Solver \veriT.

ToDo: explain the organization of the paper here..



\section{Propositional Resolution Calculus}

A \emph{literal} is a propositional variable or the negation of a propositional variable. The \emph{dual} of a literal $\ell$ is denoted
$\dual{\ell}$ (i.e. for any propositional variable $p$, $\dual{p} =
\neg p$ and $\dual{\neg p} = p$) and the \emph{underlying variable} of a literal
$\ell$ is written $|\ell|$ (i.e. $|p| = |\neg p| = p$). The set of all literals is denoted $\mathcal{L}$. A \emph{clause} is a set of literals. $\bot$ denotes the \emph{empty clause}.


\newcommand{\axiom}[1]{\hat{#1}}
\newcommand{\n}{v}
\newcommand{\raiz}[1]{\rho(#1)}

\begin{definition}[Proof] 
\label{def:proof}
A directed acyclic graph $\langle V, E, \clause \rangle$, 
where $V$ is a set of nodes and 
$E$ is a set of edges labeled by literals (i.e. $E \subset V \times \mathcal{L} \times V$ and $\n_1 \xrightarrow{\ell} \n_2$ denotes an edge from node $\n_1$ to node $\n_2$ labeled by $\ell$), is a proof of a clause $c$ iff it is
inductively constructible according to the following cases:
%
\begin{enumerate}
  \item If $\Gamma$ is a clause, $\axiom{\Gamma}$ denotes some proof $\langle \{ \n \}, \varnothing, \Gamma \} \rangle$, where $\n$ is a new node.
  \item If $\psi_L$ is a proof $\langle V_L, E_L, \clause_L \rangle$ and
    $\psi_R$ is a proof $\langle V_R, E_R, \clause_R \rangle$ and $a$ is an atom such that
    $a \in \clause_L$ and $\neg a \in \clause_R$, then
    $\psi_L \odot_a \psi_R$ denotes a proof 
    $$\langle 
    V_L \cup V_R \cup \{\n \},
    E_L \cup E_R \cup \{\n \xrightarrow{a} \raiz{\psi_L}, \n \xrightarrow{\neg a} \raiz{\psi_R}\}, 
    (\clause_L \setminus \{ a \}) \cup (\clause_R \setminus \{ \neg a\})
    \rangle$$
     where $\n$ is a new node and $\raiz{\varphi}$ denotes the root node of $\varphi$.
  \qed
\end{enumerate}
\end{definition}


\newcommand{\Vertices}[1]{V_{#1}}
\newcommand{\Edges}[1]{E_{#1}}
\newcommand{\Conclusion}[1]{c_{#1}}

\noindent
If $\n_1 \xrightarrow{\ell} \n_2$, then $\n_2$ is called a \emph{premise} of $\n_1$ and $\n_1$ is called a \emph{child} of $\n_2$. The transitive closures of the premise and child relations are, respectively, the \emph{ancestor} and \emph{descendent} relations. If $\psi = \varphi_L \odot \varphi_R$, then $\varphi_L$ and $\varphi_R$ are \emph{direct subproofs} of $\psi$. The transitive closure of the direct subproof relation is the \emph{subproof} relation.
\BWP{Before submitting, we should check which of these relations we actually use in the paper and remove the ones that are not used.}  
$\Vertices{\psi}$, $\Edges{\psi}$ and $\Conclusion{\psi}$ denote, respectively, the nodes, edges and the proven clause (conclusion) of $\psi$.

\newcommand{\Active}[2]{A_{#2}(#1)}
\begin{definition}[Active literals]
The set of active literals $\Active{\varphi}{\psi}$ of a subproof $\varphi$ of a proof $\psi$
are the labels of edges coming into $\varphi$'s root: 
$$
\Active{\varphi}{\psi} = \{\ell \ | \ \exists \varsigma \in \Vertices{\psi}. \ \varsigma \xrightarrow{\ell} \raiz{\varphi} \}
$$
\end{definition}

\begin{bwp}
Advantages of the new definition of proof: 
\begin{itemize}
\item it is similar to the implementation: hat corresponds to an axiom inference, and odot corresponds to CutIC.
\item it is simple and short.
\item it is similar to the usual notion of resolution proofs that readers already have in mind.
\end{itemize}

Disadvantage: the notion of "new node" is a bit vague. But I think we can leave it like this. Analogously, the notion of "fresh skolem symbol" is also usually left vague.

With this definition, I think it might be easier to define fixing inductively, in a manner similar to what we do in the implementation (although not necessarily immutably). We could use the notation $\psi \backslash (\varphi_1,\ldots, \varphi_n)$ to denote the result of fixing $\psi$ when the subproofs $\varphi_1, \ldots, \varphi_n$ have been marked for deletion.

In general, I have the impression that it will be easier to avoid defining or working with "broken proofs" or DAGs that are not proofs. It is probably simpler and easier to work with a pair consisting of a proof and a list of nodes/subproofs "marked for deletion", as we do in our implementation\ldots  What do you think?

Everything below this point still has to be updated to account for the new definition of proof
\end{bwp}

For any proof $\psi = \langle V,E,f_V,f_E,\rho \rangle$ on $\mathcal{A}$, the following properties
can easily be proved by induction.

\begin{property}
\label{prop:proof_leaf}
Axioms are leaves : $\Gamma \in \mathcal{A} \cap V \Leftrightarrow \forall \eta \in V ,~
(\Gamma,\eta) \notin E$.
\end{property}

\begin{property}
Every inner node $\varsigma \in V \setminus \mathcal{A}$ has exactely two premises.
\end{property}

\begin{property}
\label{prop:proof_edges}
If a node $\varsigma \in V$ has two premises $\eta_L$ and $\eta_R$ then
\begin{equation*}
f_E(\varsigma,\eta_L) = \overline{f_E(\varsigma,\eta_R)}
\end{equation*}
\end{property}

\begin{property}
\label{prop:proof_conclusion}
The conclusion of every node $\varsigma$ verifies
\begin{equation*}
  f_V(\varsigma) = \begin{cases}
    \varsigma & \varsigma \text{ has no premise} \\
    \bigcup_{(\varsigma,\eta) \in E}{f_V(\eta) \setminus f_E(\varsigma,\eta)} & \text{otherwise}
  \end{cases}
\end{equation*}
\end{property}

\begin{property}
Every active literal of a node $\eta$ belongs to $\eta$'s conclusion:
\begin{equation*}
  (\varsigma,\eta) \in E \Rightarrow f_E(\varsigma,\eta) \in f_V(\eta)
\end{equation*}
\end{property}

\begin{property}
For all node $\eta$ different from $\rho$ there is a path from $\rho$ to $\eta$.
\end{property}

\BWP{
Most of these properties are trivial. Perhaps we shouldn't mention them.
On the other hand, the converse does not seem so trivial. We would need to show that all these properties suffice.
 This lemma would be important, if we want to show a proof that fixing really converts a broken proof into a proof.
}
Conversely, if all those properties hold for a directed acyclic graph (DAG), then it is a proof,
modulo the identity of inner nodes.

\section{Basic Proof Transformations}


Proof compression algorithms presented in this paper are local graph transformations.  But many
simple graph transformations like deleting an edge may result in DAGs which aren't proofs anymore.
An important class of such transformations are those which result in DAGs verifying the property
\ref{prop:proof_edges} plus the following one.

\begin{property}
\label{prop:pseudo-proof}
Every node has at most two premises.
\end{property}

\begin{definition}{Broken Proof}
ToDo

\end{definition}


Such a DAG can be transformed into a proof by \emph{fixing} it as defined below.

\begin{definition}[Fixing]
Fixing a DAG $\langle V, E, f_V, f_E, \rho \rangle$ verifying properties \ref{prop:proof_edges} and
\ref{prop:pseudo-proof} into a proof on $\mathcal{A}$ consist in applying recursively the following
transformations until a fix-point is reached.
\begin{enumerate}
  \item Delete every node $\eta \in V \setminus \mathcal{A}$ which have no premise.
  \item For every node $\varsigma$ which have exactely one premise $\eta$, replace every incoming
    edge $(\theta,\varsigma)$ by $(\theta,\eta)$.
  \item Replace $f_V$ by a function verifying the property \ref{prop:proof_conclusion}.
  \item For every edge $(\varsigma,\eta)$ such that $f_E(\varsigma,\eta) \notin f_V(\eta)$, replace
    every edge $(\theta,\varsigma) \in E$ by $(\theta,\eta)$.
  \item Delete every node and every edge not reachable from $\rho$.
  \qed
\end{enumerate}
\end{definition}
\JB{
Do you think we need a proof of that or is it obvious ?
}


With the help of this fixing operation, the deletion of a node in a proof and the replacement of a
node by another one can easily be defined.

\begin{definition}[Deletion of a node]
Deleting a node $\eta$ in a proof $\psi$ consist in deleting every edge $(\varsigma,\eta) \in E$ and
fixing the resulting DAG. It is written $\psi[\setminus \eta]$.
\end{definition}

\begin{definition}[Replacing a node]
Replacing a node $\eta$ in a proof $\psi$ by a proof $\varphi$ with root $\rho$ consist in adding
every nodes and edges from $\varphi$, replacing every edge $(\varsigma,\eta) \in E$ by
$(\varsigma,\rho)$ and fixing the resulting DAG.  It is written $\psi[\eta \leftarrow \varphi]$.
\end{definition}

\begin{jb}
The definition of the replacement is very informal. If you prefer a more formal one, here it is :\\
Replacing a node $\eta$ in a proof $\psi = \langle V,E,f_V,f_E,\rho \rangle$ by a proof $\varphi =
\langle V_\varphi,E_\varphi,f_{V_\varphi},f_{E_\varphi},\rho_\varphi \rangle$ consits in fixing the DAG
$\langle V',E',f_V',f_E',\rho \rangle$ such that $f_V'$ is defined to verify the property
\ref{prop:proof_conclusion} and
\begin{equation*}
  V' = (V \setminus \{\eta\}) \cup V_\varphi
\end{equation*}
\begin{equation*}
  E' = (E \setminus \{(\varsigma,\eta)|\varsigma \in V\}) \cup E \cup
       \{(\varsigma,\rho_\varphi)|(\varsigma,\eta) \in E\}
\end{equation*}
\begin{equation*}
  f_V'(\varsigma,\theta) = \begin{cases}
    f_V(\varsigma,\theta) & \varsigma \in E \text{ and } \theta \in E \\
    f_{V_\varphi}(\varsigma,\theta) & \varsigma \in E_\varphi \text{ and } \theta \in E_\varphi \\
    f_V(\varsigma,\eta) & \varsigma \in E \text{ and } \theta = \rho_\varphi
  \end{cases}
\end{equation*}
\end{jb}

Finaly, for the transformations to result in proof size compression the resulting proof's conclusion
has to be equal or to subsume the original proof's conclusion. To help in proving such statement the
concepts of safe literal (introduced in \cite{RP}) and of valent literal are defined.

\BWP{In the definition of safe literal, note that $\varphi$ can be an arbitrary subproof... it is not
necessarily a single node.}

\begin{definition}[Safe literal]
In a proof $\psi$ of $\Sigma$, a literal $p$ is safe for the node $\eta$ which conclusion is $\Gamma$ if
for all proof $\varphi$ of $\Gamma \cup \{p\}$, $\psi[\eta \leftarrow \varphi]$ still proves $\Sigma$.
\end{definition}

\begin{definition}[Valent literal]
In a proof $\psi$ of $\Sigma$, a literal $a$ is valent for the node $\eta$ if $\dual{a}$ belongs to
the conclusion of $\psi[\setminus \eta]$ but not to the conclusion of $\psi$.
\end{definition}

\begin{proposition}
In a proof $\psi$, a valent literal of a node $\eta$ is an active literal of $\eta$.
\end{proposition}

\begin{proof}
If $a$ is a valent literal of $\eta$ then an edge with label $\dual{a}$ is removed by the deletion of
$\eta$ in $\psi$. The proposition states that this edge is removed by applying the step 2 of fixing
to a child of $\eta$ in $\psi$. We will prove that such an edge can't be deleted by any other step.
First this edge can't be an edge pointing to $\eta$ because step 5 of fixing deletes $\eta$.
It can't have been removed by step 1 of fixing because this step is never applied for deletion of a
single node.
It can't have been removed by step 4 because this step doesn't introduce any literal in any conclusion.
It can't have been removed by step 5 because the edge has to be reachable from the root of $\psi$.
Finally, the step 2 can't be applied to any other node but the children
of $\eta$. \qed
\end{proof}

\JB{
This last proposition and its proof are very important for LUniv. Should I make it a theorem ?
}


\section{LowerUnits}

When a node $\eta$ as more than one child in a proof $\psi$ it might be convenient to factor the
corresponding resolutions. Lowering $\eta$ is such a factorisation. A new equivalent proof is
constructed by removing $\eta$ from $\psi$, fixing the resulting DAG and then reintroducing $\eta$
at the bottom of the proof. Formaly, a node $\eta$ in a proof $\psi$ of $\Gamma$ can be lowered if
there exists a proof $\psi'$ of $\Gamma$ and a literal $a$ such that $\psi' = \psi[\setminus \eta]
\odot_a \eta$.

These idea has been introduce in \cite{LURPI} for the {\LowerUnits} algorithm. Units are nodes with a
conclusion consisting of only one literal. Such nodes can always be lowered. The proposed algorithm
lowers every unit with more than one child. Care is taken to reintroduce units in an order
corresponding to the transitive closure of the premise relation : if a unit $\eta$ is a parent of a
unit $\varsigma$ then $\eta$ has to be reintroduced after (ie below) $\varsigma$.

A possible presentation of {\LowerUnits} is shown in Algorithm \ref{algo:LU}. Units are collected during a
first traversal. As this traversal is bottom-up, units are stored in a queue. The traversal could
have been top-down and units stored in a stack. Units are effectively removed during a second,
top-down traversal. The last step is the reintroduction of units.

\begin{algorithm}[bt]
  Units $\leftarrow \varnothing$ \;

  \For{every node $\eta$ in a bottom-up traversal}{
    \If{$\eta$ is a unit with more than one child}{Enqueue $\eta$ in Units \; }
  }

  \For{every node $\varsigma$ in a top-down traversal}{
    \uIf{a premise of $\varsigma$ belongs to Units}{
      Replace $\varsigma$ by the other premise \; }
    \Else{
      Recompute $\varsigma$'s conclusion based on the current premises \; }
  }

  $\rho \leftarrow$ the new root of the proof \;
  \For{every unit $\upsilon$ in Units}{
    Let $\{u\}$ be $\upsilon$'s conclusion \;
    \lIf{$\rho$'s conclusion contains $\dual{u}$}{
    $\rho \leftarrow \rho \odot_u \upsilon$ \;}
  }

  \label{algo:LU}
  \caption{\LowerUnits}
\end{algorithm}

LU has been successfully composed with the \texttt{RecyclePivotsWithIntersection} (RPI) algorithm
presented in \cite{LURPI}. Both sequential compositions achieve very good compression ratio in
reasonnable amount of time. Unfortunately, none of them is always better than the other and there is
actually no heuristic to choose which one to apply a priori.

\section{LowerUnivalents}

\begin{jb}
This is the main section. First we present formaly the principles of the algorithm. Then the partial
regularization. Then the algorithm. Then the proof it's always better than LU (and LUnivRPI always
better than LU.RPI).

This is still a draft that have to be completely rewritten.
\end{jb}

{\LowerUnits} obviously doesn't lower every lowerable subproofs. In particular, it doesn't take into
account the already lowered subproofs. For instance, if a unit $\upsilon$ proving $\{a\}$ has
already been lowered, a subproof $\eta$ of $\{\dual{a},b\}$ may be lowered too and reintroduced
above $\upsilon$. To state which conditions $\eta$ must satisfy to be lowered without changing the
proof's conclusion, the lower problem has to be redefined as follow.

\begin{definition}[Lowerable subproof]
Given a proof $\psi$ of $\Gamma$ and a subproof $\varphi$, a subproof $\eta$ from $\varphi$ is lowerable
below $\varphi$ if there exists a literal $\ell$ and a proof $\psi'$ of $\Gamma' \subseteq
\Gamma$ such that
\begin{equation}
  \psi' = \rn{\psi}{\varphi}{\left(\dn{\varphi}{\eta}\right) \odot_\ell \eta}
\end{equation}
\end{definition}
The previous definition is very general. The lowerable subproof introduction presented in the
previous Section can be obtained when $\varphi = \psi$. In the context of proof compression,
lowering a subproof is wanted to reduce the number of $\eta$'s root's children. For instance, it
could be required that every path from the root of $\psi$ to the root of $\eta$ goes through the
root of $\varphi$. In this case, $\eta$'s root is guaranted to have only one child in $\psi'$.

\begin{definition}[Lowered part]
If a proof $\psi$ can be written as
\begin{equation}
  \psi = \eta_1 \odot_{\ell_1} ( \eta_2 \odot_{\ell_2} ( \ldots (\eta_n \odot_{\ell_n} \varphi)
          \ldots ))
\end{equation}
then the pair $\langle L,\Delta \rangle$ in which $L$ is the sequence $(\eta_1,\ldots,\eta_n)$ of
proofs and $\Delta$ the sequence $(\ell_1,\ldots,\ell_n)$ of literals is called a lowered part of
$\psi$ below $\varphi$.
\end{definition}
Once again this is a broad definition. In the context of proof compression the root of each proof
$\eta_i \in L$ would usualy have exactely one child in $\psi$.

\begin{definition}[Univalent subproof]
A subproof $\eta$ of $\Gamma$ in a proof $\psi$ is \emph{univalent} w.r.t. a set $\Delta$ of literals if $\eta$ has
exactely one valent literal $\ell$ in $\psi$ and $\Gamma \subseteq \Delta \cup \left\{ \ell
\right\}$.
\end{definition}

\begin{proposition}
If $\langle L,\Delta \rangle$ is a lowered part of $\psi$ below $\varphi$ and $\eta$ a subproof of
$\varphi$ which is univalent in $\varphi$ w.r.t. $\Delta$, then $\eta$ is lowerable in $\psi$ below $\varphi$.
\end{proposition}

\begin{proof}
Let call $\Gamma$ the conclusion of $\varphi$.  The only valent literal of $\eta$ in $\varphi$ being
$\ell$, the conclusion of $\dn{\varphi}{\eta}$ is a subset of $\Gamma \cup \{\dual{\ell}\}$. As the
conclusion of $\eta$ is a subset of $\Delta \cup \{\ell\}$, the conclusion of $\left(
\dn{\varphi}{\eta} \right) \odot_\ell \eta$ is a subset of $\Gamma \cup \Delta$. Finaly, $\Delta$ is
a set of safe literals for $\varphi$ in $\psi$. \qed
\end{proof}


\section{Experiments}

\begin{jb}
LU vs LUniv ; RPI[3]LU vs RPI[3]LUniv ; LUnivRPI vs LU.RPI.
\end{jb}


\BWP{1) If possible, add RP and RPI to this table too. Otherwise we will only be comparing our own algorithms.}
\BWP{2) Also, it seems that we are only showing the non-sequential combinations here (or is RPILU actually "RPI.LU"??). If we could also show the sequential combinations, we would compare their speeds and show that the non-sequential combinations are faster, and that LUniv contributed to this.}
\begin{table}[ht]
  \centering
  \begin{tabular}{lrrr}
    \toprule
    \multirow{2}{*}{Algorithm} & Nodes       & Axioms      & \multirow{2}{*}{Speed} \\
                               & compression & compression & \\
    \midrule
    LU         &  7.5 \% &  0.0 \% & 21.7 n/ms \\
    LUniv      &  8.0 \% &  0.8 \% &  7.6 n/ms \\
    RPILU      & 22.3 \% &  3.6 \% &  8.3 n/ms \\
    RPILUniv   & 22.4 \% &  3.6 \% &  5.0 n/ms \\
    LURPI      & 21.9 \% &  3.1 \% & 15.2 n/ms \\
    LUnivRPI   & 22.3 \% &  3.6 \% &  8.1 n/ms \\
    Best LU    & 22.4 \% &  3.7 \% &  5.4 n/ms \\
    Best LUniv & 22.5 \% &  3.8 \% &  3.1 n/ms \\
    \bottomrule
  \end{tabular}
  \caption{Total compression ratios}
\end{table}

\BWP{3) So, LUniv is about 3 times slower than LU. Can we blame implementation details (e.g. the fact that we use lists instead of sets for conclusion clauses) for this?}

\BWP{4) I think you forgot to push the files with charts}


\include{LU_charts}
\include{RPILU_charts}
\include{LURPI_charts}
\include{best_charts}

\section{Conclusions}

\bibliographystyle{splncs}
\bibliography{../biblio}

\end{document}
